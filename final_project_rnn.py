# -*- coding: utf-8 -*-
"""FINAL_PROJECT_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv5Lq9oIrxNBS9nvhSW3DRrnSWMrX4U_
"""

# =========================
# STEP 1: IMPORT LIBRARIES
# =========================
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, LSTM
from tensorflow.keras.callbacks import EarlyStopping


# Load CSV
df = pd.read_csv(
    "/content/household_power_consumption.csv",
    sep=',', # Changed separator from ';' to ','
    na_values='?'
)

# Use the 'time' column directly for Datetime conversion and set as index
df['Datetime'] = pd.to_datetime(
    df['time'], # Changed from df['Date'] + ' ' + df['Time']
    errors='coerce'
)

df.drop(['time'], axis=1, inplace=True) # Dropped 'time' column
df.set_index('Datetime', inplace=True)


# Convert all columns to float
df = df.astype(float)

# Fill missing values
df.fillna(method='ffill', inplace=True)

# Capitalize column names to match the 'features' list
df.columns = [col.capitalize() for col in df.columns]

print(df.head())


# =========================
# STEP 3: FEATURE SELECTION
# =========================
features = [
    'Global_active_power',
    'Global_reactive_power',
    'Voltage',
    'Global_intensity',
    'Sub_metering_1',
    'Sub_metering_2',
    'Sub_metering_3'
]

data = df[features]


# =========================
# STEP 4: TRAIN TEST SPLIT
# =========================
train_size = int(len(data) * 0.8)
train_data = data[:train_size]
test_data = data[train_size:]


# =========================
# STEP 5: SCALING
# =========================
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_data)
test_scaled = scaler.transform(test_data)


# =========================
# STEP 6: SEQUENCE CREATION
# =========================
def create_sequences(data, seq_len=96):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, 0])
    return np.array(X), np.array(y)

SEQ_LEN = 96

X_train, y_train = create_sequences(train_scaled, SEQ_LEN)
X_test, y_test = create_sequences(test_scaled, SEQ_LEN)


# =========================
# STEP 7: TRANSFORMER MODEL
# =========================
def transformer_model(input_shape):
    inputs = Input(shape=input_shape)

    x = MultiHeadAttention(num_heads=4, key_dim=32)(inputs, inputs)
    x = Dropout(0.1)(x)
    x = LayerNormalization()(x)

    x = Dense(64, activation='relu')(x)
    outputs = Dense(1)(x)

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='mse')
    return model

model = transformer_model((X_train.shape[1], X_train.shape[2]))


# =========================
# STEP 8: TRAIN MODEL
# =========================
early_stop = EarlyStopping(patience=5, restore_best_weights=True)

model.fit(
    X_train,
    y_train,
    validation_split=0.2,
    epochs=30,
    batch_size=64,
    callbacks=[early_stop]
)


# =========================
# STEP 9: EVALUATION
# =========================
predictions = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print("Transformer RMSE:", rmse)


# =========================
# STEP 10: BASELINE LSTM
# =========================
lstm = tf.keras.Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(1)
])

lstm.compile(optimizer='adam', loss='mse')

lstm.fit(
    X_train,
    y_train,
    validation_split=0.2,
    epochs=20,
    batch_size=64
)

lstm_preds = lstm.predict(X_test)
lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_preds))
print("LSTM RMSE:", lstm_rmse)


# =========================
# STEP 11: SAVE MODEL
# =========================
model.save("transformer_household_power_model.h5")